---
title: "Report of MA678 Midterm Project"
author: "Tong Sun"
date: "12/1/2021"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(highcharter)
library(lubridate)
library(stringr)
library(xts)
library(lme4)
library(lattice)
library(gridExtra)
library(arm)
library(ggpubr)
data<-read.csv("googleplaystore.csv")
```

## Abstract

Google Play is the official app store of Android, Google's mobile platform. It allows people to view applications and load of content before downloading anything on their devices. The Play Store apps data has enormous potential to drive app-making businesses to success. But there still are some apps which have majority of reviews with fewer installs. So here a question comes out -- what factors might influence the number of users' reviews ? I used several factors and set up a multilevel model to answer this question, making some suggestions for future app-making businesses. Additionally, it indexed important information about Android apps. The model shows that the number of reviews is slightly different between categories. This report consists 4 main parts: Introduction, Method, Result and Conlusion.

## Introduction

Google Play indexes important information about Android apps, including ratings, alternative suggestions,user reviews and other descriptions of apps. It uses sophisticated modern-day techniques (like dynamic page load) using JQuery making scraping more challenging. The Play Store apps data has enormous potential to drive app-making businesses to success. Actionable insights can be drawn for developers to work on and capture the Android market! So each app has its own features and some similarities with others as well, such as the number of installs, types and the price of apps. And some features may lead the apps to be more out-standing.

So here I would use multilevel regression to see how different their number of reviews are between each category.Before that, I cleaned the data and added with some new columns to see which factors are more significant.

## Method

# Data Cleaning and Processing

The main data set is published on [Kaggle: Google Play Store Dataset](https://www.kaggle.com/lava18/google-play-store-apps).

Because the data set looks so messy and includes different types of variables, firstly I made lots of steps in data cleaning.I checked the types of the data and variables and changed all factor variables to be numeric, eliminating lots of symbols (such as "M"and currency symbol) as well. Next I replaced or removed all no-info items in each column because they didn't have any practical meaning. Finally, I changed 'Last Updated' column to date format.

# Description of Data

| column names      | explanation |
| :--:              | :----- |
| App               | Application name|
| Rating            | Overall user rating of the app (as when scraped) |
| Reviews           | Number of user reviews for the app (as when scraped) |
| Size              | Size of the app (as when scraped) |
| Installs          | Number of user downloads for the app (as when scraped) |
| Type              | Paid of Free|
| Price             | Price of the app (as when scraped) |
| Content Rating    | Age group the app is targeted at-Children/Mature 21+/Adult |
| Genres            | An app can belong to multiple genres (apart from its main category) |

# EDA

Here I would like to draw some plots to see the relationship between reviews and other variables, since my question is how some factors effect the number of reviews.

```{r}
p1<-ggplot(data = newdata)+
  aes(log_reviews,log_installs)+
  geom_point(aes(color = Category),alpha = 0.8)+
  labs(title="grades of installs vs number of reviews",x="log(grades of installs)",y="log(number of reviews)")+
  geom_smooth(aes(color = Category),method = "lm",se=F)+
  facet_wrap(~Content.Rating)
p4<-ggplot(data.frame(newdata$log_reviews,newdata$log_installs,newdata$Category))+geom_point()+
  aes(x=newdata$log_installs, y=newdata$log_reviews, color=newdata$Category)+
  stat_summary(fun = "mean", geom = "line", alpha=0.3)+
  stat_summary(fun = "mean", geom = "line", lty=2, aes(group=1),color="black")+
  ylab("log number of reviews")+xlab("log number of installs")+theme(legend.position = "none")
ggarrange(p1,p4)
```

Figure 1 shows the relationship between the number of installs and reviews. In order to make the outcome more clearly, I made six facets by column 'Content.Rating'. But it seems like there is not too much significant relationship between these two variables, because the smoothed line are all in the same intercept and slope. Similarly, the plot on the right shows the same result -- all the lines in the plot have the similar trend by default. So I guess the variable 'log_installs' should not be included in our model.

```{r}
p2<-ggplot(data = newdata)+
  aes(log_reviews,Size)+
  geom_point(aes(color = Category),alpha = 0.8)+
  labs(title="Size vs number of reviews",x="Size",y="log(number of reviews)")+
  geom_smooth(aes(color = Category),method = "lm",se=F)+
  facet_wrap(~Content.Rating)
p5<-ggplot(data.frame(newdata$log_reviews,newdata$Size,newdata$Category))+geom_point()+
  aes(x=newdata$Size, y=newdata$log_reviews, color=newdata$Category)+
  stat_summary(fun = "mean", geom = "line", alpha=0.3)+
  stat_summary(fun = "mean", geom = "line", lty=2, aes(group=1),color="black")+
  ylab("log number of reviews")+xlab("Size")+theme(legend.position = "none")
ggarrange(p2,p5)
```

Figure 2 shows the relationship between size and the number of reviews. Unlike the 'installs', the relationship here varies obviously between categories. There should be random intercept and random slope at the same time. For the plot on the right, I found that for some categories of apps, the number of reviews varies dramatically with the size of the application.

```{r}
p3<-ggplot(data = newdata)+
  aes(log_reviews,Rating)+
  geom_point(aes(color = Category),alpha = 0.8)+
  labs(title="Rating vs number of reviews",x="Rating",y="log(number of reviews)")+
  geom_smooth(aes(color = Category),method = "lm",se=F)+
  facet_wrap(~Content.Rating)
p6<-ggplot(data.frame(newdata$log_reviews,newdata$Rating,newdata$Category))+geom_point()+
  aes(x=newdata$Rating, y=newdata$log_reviews, color=newdata$Category)+
  stat_summary(fun = "mean", geom = "line", alpha=0.3)+
  stat_summary(fun = "mean", geom = "line", lty=2, aes(group=1),color="black")+
  ylab("log number of reviews")+xlab("Rating")+theme(legend.position = "none")
ggarrange(p3,p6)
```

Figure 3 shows the relationship between rating and the number of reviews. The result is the same as the size's. The differences between categories are so significant. Also different categories have different slopes and intercepts. Therefore, I think the variables 'Size' and 'Rating' should be taken into account when I fit the model.

# Model Fitting

After deleting lost data and duplicated rows, transforming log of column 'Installs' and 'Reviews' (because I found they all have a large scale with a long tale from the histogram plots showed above), I got 7369 observations of 15 variables. Here I only analysis those 'Type' == "Free",which has a larger proportion. 

```{r}

```

Considering different categories, I used multilevel model to fit the data. From the EDA part, I found that for the 'log_installs' predictor, the differences between each category was not so significant. So I did not choose 'log_installs' as one of model's predictor.

Here I chose continuous variables -- 'Size' and 'Rating' as model's predictors. From EDA part above, I guessed that for 'Size' and 'Rating' predictors, it should be random intercept and random slope model. To be more convinced, I still did three different models and used anova for model selecting (choosing the one has the smallest AIC). Below is the function:

```{r}
model.2<-lmer(log_reviews ~ Rating + Size + (1+Rating|Category) + (1+Size|Category) , data = newdata)
summary(model.2)
```

To see the fixed effects below:

|                |Estimate   |Std. Error  |t value |
|:---:           |:---:      |:---:       |:---:   |
|(Intercept)     |1.42       |0.52        |2.75    |
|Rating          |1.24       |0.16        |7.72    |
|Size            |0.05       |0.01        |8.48    |

# Model results

As we can see the results of fixed effects above, here I only take one of those categories into account. For ART_AND_DESIGN category, we can conclude this formula:

$$log(reviews)= 4.70 + 0.64\cdot Rating + 0.07\cdot Size$$

All the parameters of three predictors are all bigger than 0 (except for the parameter of 'Size' under AUTO_AND_VEHICLES category), which means they all have positive impact on number of reviews. For each 1% difference in the number of installs, the predicted difference in reviews is 0.95%. About the reason of the negative parameter of 'Size' under AUTO_AND_VEHICLES category, I think it does not have so much meaning. Because the abstract value of this parameter is almost 0. 

In addition, we can see differences between each category. Here I took three of them as example. Let's look at their parameters of these predictors as follow:

|                        |(Intercept) |Rating       |Size  |
|:---:                   |:---:       |:---:        |:---: |
|ENTERTAINMENT           |-7.25       |2.84         |0.05  |
|EVENTS                  |7.29        |0.18         |0.02  |
|MEDICAL                 |6.42        |0.27         |0.01  |

For different categories, the influence of each predictor is always not the same. For ENTERTAINMENT, I think whether people will download the app depends more on how people rate it, and because it's entertainment, people are more likely to decide whether to download a non-essential app because of the size of the device's memory. So the parameters of 'Rating' and 'Size' are bigger than other categories. For EVENTS, the parameter of 'Rating' is small, this might because these apps are mostly nonfiction apps, and people rarely rate them. And for MEDICAL, since apps in this category are essential to people's lives, the number of times people review them is less dependent on other users' ratings and the size of the app itself.

# Model Validation

```{r echo=FALSE, fig.height=2.5, fig.width=6, fig.cap="Residual plot and Q-Q plot."}
re <- plot(model.2)
qq <- qqmath(model.2)
grid.arrange(re,qq,nrow=1)
```

```{r echo=FALSE, fig.height=2, fig.width=4, fig.cap="Residuals vs Leverage."}
ggplot(data.frame(lev=hatvalues(model.2),pearson=residuals(model.2,type="pearson")),
      aes(x=lev,y=pearson)) +
    geom_point() +
    theme_bw()
```

From the residual plot,in Figure 4, I find that the mean of residuals is almost zero.From the Q-Q plot, I find that most of dots are on the lines so the normality is good. From the leverage plot in Figure 5, we can see that there are not obvious leverage point, except for some points on the right.This refers to the extent to which the coefficients in the regression model would not change if a particular observation was removed from the dataset.

## Conclusion

From the estimates above, I would draw the conclusion as follow. The parameters of variables shows that "Rating" and "Size" both have a positive influence on the number of reviews. The higher level from people's rating, which means people who used this app before have more sense about it, they would like to recommend it to others, so they are willing to share their feelings on social platforms. In addition, for the "Size" factor, the larger size the app has, the more probably people will write reviews for it. By default, the reason of it is those apps with larger size may include more information and more utility features, in that case there will be more people downloading and using them. Therefore there will be more reviews of them, but the influence of size is not so significant comparing with rating.

For the purpose of giving suggestions for future app-making businesses, I 

## Citation


\newpage
## Appendix

# More EDA

```{r}
p7<-hist(data.clean$Installs)
p8<-hist(data.clean$Reviews)
grid.arrange(p7,p8)
```
\newpage
## Full Results

Random effects of model
```{r echo=FALSE}
ranef(model.2)
```

Fixed effects of model
```{r echo=FALSE}
fixef(model.2)
```

Coefficients of model
```{r echo=FALSE}
coef(model.2)
```


