---
title: "Report of MA678 Midterm Project"
author: "Tong Sun"
date: "12/1/2021"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(highcharter)
library(lubridate)
library(stringr)
library(xts)
library(lme4)
library(lattice)
library(gridExtra)
data<-read.csv("googleplaystore.csv")
```

## Abstract

Google Play is the official app store of Android, Google's mobile platform. It allows people to view applications and load of content before downloading anything on their devices. The Play Store apps data has enormous potential to drive app-making businesses to success. But there still are some apps which have majority of reviews with fewer installs. So here a question comes out -- what factors might influence the number of users' reviews ? I used several factors and set up a multilevel model to answer this question, making some suggestions for future app-making businesses. Additionally, it indexed important information about Android apps. The model shows that the number of reviews is slightly different between categories. This report consists 4 main parts: Introduction, Method, Result and Conlusion.

## Introduction

Google Play indexes important information about Android apps, including ratings, alternative suggestions,user reviews and other descriptions of apps. It uses sophisticated modern-day techniques (like dynamic page load) using JQuery making scraping more challenging. The Play Store apps data has enormous potential to drive app-making businesses to success. Actionable insights can be drawn for developers to work on and capture the Android market! So each app has its own features and some similarities with others as well, such as the number of installs, types and the price of apps. And some features may lead the apps to be more out-standing.

So here I would use multilevel regression to see how different their number of reviews are between each category.Before that, I cleaned the data and added with some new columns to see which factors are more significant.

## Method

# Data Cleaning and Processing

The main data set is published on [Kaggle: Google Play Store Dataset](https://www.kaggle.com/lava18/google-play-store-apps).

Because the data set looks so messy and includes different types of variables, firstly I made lots of steps in data cleaning.I checked the types of the data and variables and changed all factor variables to be numeric, eliminating lots of symbols (such as "M"and currency symbol) as well. Next I replaced or removed all no-info items in each column because they didn't have any practical meaning. Finally, I changed 'Last Updated' column to date format.

# Description of Data

| column names      | explanation |
| :--:              | :----- |
| App               | Application name|
| Rating            | Overall user rating of the app (as when scraped) |
| Reviews           | Number of user reviews for the app (as when scraped) |
| Size              | Size of the app (as when scraped) |
| Installs          | Number of user downloads for the app (as when scraped) |
| Type              | Paid of Free|
| Price             | Price of the app (as when scraped) |
| Content Rating    | Age group the app is targeted at-Children/Mature 21+/Adult |
| Genres            | An app can belong to multiple genres (apart from its main category) |

# EDA


# Model Fitting

After deleting lost data and duplicated rows, transforming log of column 'Installs' and 'Reviews' (because I found they all have a large scale with a long tale from the histogram plots showed above), I got 7369 observations of 15 variables. Here I only analysis those 'Type' == "Free",which has a larger proportion. 

```{r}

```

Considering different categories, I used multilevel model to fit the data. From the EDA part, I found that for the 'log_installs' predictor, the differences between each category was not so significant. So I did not choose 'log_installs' as one of model's predictor.

Here I chose continuous variables -- 'Size' and 'Rating' as model's predictors. From EDA part above, I guessed that for 'Size' and 'Rating' predictors, it should be random intercept and random slope model. To be more convinced, I still did three different models and used anova for model selecting (choosing the one has the smallest AIC). Below is the function:

```{r}
model.2<-lmer(log_reviews ~ Rating + Size + (1+Rating|Category) + (1+Size|Category) , data = newdata)
summary(model.2)
```

To see the fixed effects below:

|                |Estimate   |Std. Error  |t value |
|:---:           |:---:      |:---:       |:---:   |
|(Intercept)     |1.42       |0.52        |2.75    |
|Rating          |1.24       |0.16        |7.72    |
|Size            |0.05       |0.01        |8.48    |

# Model results

As we can see the results of fixed effects above, here I only take one of those categories into account. For ART_AND_DESIGN category, we can conclude this formula:

$$log(reviews)= 4.70 + 0.64\cdot Rating + 0.07\cdot Size$$

All the parameters of three predictors are all bigger than 0 (except for the parameter of 'Size' under AUTO_AND_VEHICLES category), which means they all have positive impact on number of reviews. For each 1% difference in the number of installs, the predicted difference in reviews is 0.95%. About the reason of the negative parameter of 'Size' under AUTO_AND_VEHICLES category, I think it does not have so much meaning. Because the abstract value of this parameter is almost 0. 

In addition, we can see differences between each category. Here I took three of them as example. Let's look at their parameters of these predictors as follow:

|                        |(Intercept) |Rating       |Size  |
|:---:                   |:---:       |:---:        |:---: |
|ENTERTAINMENT           |-7.25       |2.84         |0.05  |
|EVENTS                  |7.29        |0.18         |0.02  |
|MEDICAL                 |6.42        |0.27         |0.01  |

For different categories, the influence of each predictor is always not the same. For ENTERTAINMENT, I think whether people will download the app depends more on how people rate it, and because it's entertainment, people are more likely to decide whether to download a non-essential app because of the size of the device's memory. So the parameters of 'Rating' and 'Size' are bigger than other categories. For EVENTS, the parameter of 'Rating' is small, this might because these apps are mostly nonfiction apps, and people rarely rate them. And for MEDICAL, since apps in this category are essential to people's lives, the number of times people review them is less dependent on other users' ratings and the size of the app itself.

# Model Validation

```{r echo=FALSE, fig.height=2.5, fig.width=6, fig.cap="Residual plot and Q-Q plot."}
re <- plot(model.2)
qq <- qqmath(model.2)
grid.arrange(re,qq,nrow=1)
```

```{r echo=FALSE, fig.height=2, fig.width=4, fig.cap="Residuals vs Leverage."}
ggplot(data.frame(lev=hatvalues(model.2),pearson=residuals(model.2,type="pearson")),
      aes(x=lev,y=pearson)) +
    geom_point() +
    theme_bw()
```


## Conclusion


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
